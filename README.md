# ğŸ“˜ LLM Tokenizer from Scratch (Python)

> Understanding how Large Language Models preprocess text by building a tokenizer from the ground up.

---

## ğŸš€ Overview

This project implements a **word-level tokenizer from scratch** in Python to understand how Large Language Models (LLMs) process text before training or inference.

It demonstrates:

- ğŸ”¹ Regex-based tokenization  
- ğŸ”¹ Vocabulary construction from corpus text  
- ğŸ”¹ Encoding text â†’ token IDs  
- ğŸ”¹ Decoding token IDs â†’ readable text  
- ğŸ”¹ Handling unknown tokens & sequence boundaries  

ğŸ§­ This project is part of a deeper exploration into **LLM internals**, with **Byte Pair Encoding (BPE)** planned next.

---

## âœ¨ Features

âœ… Tokenizes text into words & punctuation  
âœ… Builds vocabulary from corpus text  
âœ… Handles unknown tokens with `<UNK>`  
âœ… Appends `<EOS>` (end-of-sequence) marker  
âœ… Encodes text â†’ numeric token IDs  
âœ… Decodes IDs â†’ readable text  
âœ… Clean modular architecture  

---

## ğŸ§  How It Works

### 1ï¸âƒ£ Tokenization

Text is split into tokens using regex:

- words  
- punctuation  
- whitespace removed  

**Example**

```
Hello everyone, it's nice to meet you.
```

â¬‡

```
["Hello", "everyone", ",", "it", "'", "s", "nice", "to", "meet", "you", "."]
```

---

### 2ï¸âƒ£ Vocabulary Building

From training text:

- unique tokens are extracted  
- special tokens are added:
  - `<UNK>` â†’ unknown tokens  
  - `<EOS>` â†’ end of sequence  
- tokens are mapped to integer IDs  

---

### 3ï¸âƒ£ Encoding

Text â†’ Tokens â†’ IDs

```
Hello everyone!
```

â¬‡

```
[12, 45, 7, 1]
```

Where `1` may represent `<EOS>`.

---

### 4ï¸âƒ£ Decoding

IDs â†’ Tokens â†’ Text

Restores readable text with proper punctuation spacing.

---

## ğŸ“ Project Structure

```
src/
  helper.py        # tokenization & text reconstruction
  vocab.py         # vocabulary builder
  tokenizer.py     # encode/decode logic
  main.py          # example usage

requirements.txt
.gitignore
```

---

## ğŸ§© Module Overview

### ğŸ“„ helper.py
- `tokenize_text()` â†’ splits text into tokens  
- `tokenized_text_to_str()` â†’ reconstructs text  

### ğŸ“„ vocab.py
Builds vocabulary from training corpus.

### ğŸ“„ tokenizer.py
Implements `SimpleTokenizer`:

- stores vocabulary  
- encodes text â†’ ids  
- decodes ids â†’ text  

### ğŸ“„ main.py
Demonstrates end-to-end usage.

---

## â–¶ï¸ Running the Project

### 1ï¸âƒ£ Activate environment

```bash
source .venv/bin/activate
```

### 2ï¸âƒ£ Run the program

```bash
python -m src.main
```

---

## ğŸ§ª Example Output

```
Hello everyone, it's nice to meet you. into tokens:
[45, 78, 9, 23, ...]

after decoding:
Hello everyone, it's nice to meet you.
```

---

## ğŸ¯ Learning Goals

This project demonstrates:

- how tokenizers prepare text for LLMs  
- vocabulary creation & token mapping  
- sequence boundary handling  
- modular Python design  

---

## ğŸ”œ Next Steps

Planned improvements:

- âœ… Byte Pair Encoding (BPE) tokenizer  
- subword tokenization  
- vocabulary size control  
- padding & batching support  
- tokenizer serialization  
- performance optimizations  

---

## ğŸ“š Why This Matters

Understanding tokenization helps explain:

- how LLMs understand text  
- why unknown words disappear  
- how vocabulary size affects models  
- how subwords enable generalization  

---

## ğŸ›  Built With

- Python 3  
- Regex (`re`)  
- Standard library only  

---

## ğŸ‘¤ Author

**Nikhil Mishra**  
Learning LLM internals by building components from scratch.

---

## â­ If you found this useful

Consider starring â­ the repository to follow future improvements.
